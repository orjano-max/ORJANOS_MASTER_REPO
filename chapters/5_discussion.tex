\chapter{Discussion}\label{sec:Discussion}
This chapter discusses different aspects around the methodological choises of this thesis. Starting with the choice of ROS 2 distributions in section \ref{sec:D:ROS2Distros}. Then, aspects of the Navigational system will be discussed, from the hardware to the methodological choices in section \ref{sec:D:AutonomousNavigaion}. Next, the pick and place system is discussed in section \ref{sec:D:PickAndPlace}, before the top-level system will be discussed in section \ref{sec:D:TopLevel}.

 % During the course of this project, an autonomous UGV for warehouse pick and place application was set up. Necessary mechanical modifications were done to the UGV platform in order to accommodate extra sensors and a robotic manipulator. Then, sensors and algorithms were configured to build a demonstration of how a warehouse application system could work with an autonomous UGV. Testing was then done separately on both the the autonomous navigation system and the pick and place system. Finally, the entire pipeline was tested, where autonomous navigation, computer vision and robot manipulation works together to achieve a warehouse automation task.

\section{ROS 2 Distributions} \label{sec:D:ROS2Distros}
There are some aspects to discuss regarding the choice of ROS 2 distributions during this project. Firstly, the Galactic Geochelone is not a Long Term Supported (LTS) release, reached its End of Life (EOL) December 2022 \cite{ROS2distros}, despite being newer than Foxy Fitzroy. Using a ROS 2 distribution that has reached it's EOL is usually avoided. The reasoning behind using Galactic Geochelone is based in on the fact that features too new for Foxy Fitzroy were used. It was not possible to upgrade the Ubuntu version to allow the use of the latest LTS release, Humble Hawksbill, as it would crash with another thesis.
 
Secondly, using two different ROS 2 distributions in one robotic system is not recommended in any way. In the case of this project, a lot of time was put into trying to get the Husky A200 UGV to give acceptable performance on the ROS 2 Galactic Geochelone distribution without success. And Foxy was therefore tested as a last resort to see if the robotic system would work. There seemed to be no issues with this solution during testing, however, it is bad practice and should be avoided.


\section{Autonomous Navigation} \label{sec:D:AutonomousNavigaion}

\subsection{Mobile Robot} \label{sec:D:AN:MobileRobot}
The Husky A200 UGV proved to be a practical mounting platform for developers to mount the equipment of their choice. However, there are some aspects of this UGV that should be discussed. The Husky A200 is a skidding robot (explained in section \ref{sec:T:AN:MRD:SkiddingRobots}), meaning that the robot will skid like a war-tank when turning. This is a major drawback when designing an autonomous robot since the skidding behaviour have the potential to massively reduce the performance of the robot's odometry system.

\subsection{Perception} \label{sec:D:AN:Perception}
The Ouster OS1 LiDARs performance is affected by the high mounting location and it's relatively poor minimum range. Ouster states the minimum range of the LiDAR to be $r_{min}=0.8[m]$ \cite{OS1_datasheet}. This means that all objects within a range of $r_{min}$ from the sensor will not be detected.

The high mounting position creates a large blind-zone around the robot. This blind zone is illustrated in figure \ref{fig:D:AN:P:LidarShadow}, where it can be seen that a LiDAR mounted at a height $h[m]$ above the ground will cast a shadow with a radius $r[m]$ around the LiDAR. Notice that the orange box in figure \ref{fig:D:AN:P:LidarShadow} will not be detected by the LiDAR.

\begin{figure}[ht]
  \centering
  \includesvg[width = 0.5\textwidth]{Figures/figLiDAR_Shadow.drawio.svg}
  \caption{Illustration showing the blind-zone of the LiDAR based on it's mounting height, $h$. The green box represent the sensor, the red triangles represents the sensor's vertical field of view, and the orange box represents an object in the blind-zone of the sensor. $\theta$: half of vertical field of view. $r$: radius of shadow cast on ground.}
  \label{fig:D:AN:P:LidarShadow}
\end{figure}

The radius of the shadow cast by the LiDAR in figure \ref{fig:D:AN:P:LidarShadow}, can be mathematically described as follows. For a LiDAR mounted at a height $h[m]$ above ground and vertical field of view $2\cdot\theta[rad]$ the radius of the shadow cast on the ground, $r[m]$, is given as

\begin{equation} \label{eq:LidarShadow}
    r = \frac{h}{\tan{\theta}}[m].
\end{equation}

Ouster claims a vertical field of view of $33.2^\circ \approx\frac{\pi}{12}[rad]$ \cite{OS1_datasheet} which translates to $\theta\approx\frac{\pi}{12}[rad]$. Using equation \ref{eq:LidarShadow}, with a sensor height of $1.025[m]$, the radius of the shadow cast becomes $3.825[m]$.

The use of the ROS2 package PointCloud to LaserScan, described in section \ref{sec:M:AN:Perception} is considered a success as this resulted in a 2D laser scan that took all relevant obstacles into account. This, in turn, means that a 2D map constructed from this laser scan will take all relevant obstacles into account.

\subsection{Navigation}\label{sec:D:AN:Navigation}
Autonomous navigation performance seemed to suffer the consequences of the skidding robot design of the Husky A200, discussed in section \ref{sec:D:AN:MobileRobot}. Although the robot would successfully navigate around the environment issues occurred during hard turns. This was especially prominent in situations where the goal position had been reached, but the robot would need to spin in order to reach the correct orientation. The robot would become disoriented from the spinning action, and since the default recovery action in NAV2 is spinning, it would end up spinning in a seemingly endless dance.

\subsubsection{Collision Avoidance}
Collision avoidance would suffer from the large LiDAR blind zones discussed in section \ref{sec:D:AN:Perception}. If, for example a wall was within the minimum range of the LiDAR, the robot would have no means of detecting the wall. This could result in the robot sometimes ramming itself into walls and trying to drive through them. Objects located too low and too close could get run over due to the mounting height. A solution to this issue is presented by Didrik Robsrud in his thesis on Radar and LiDAR sensor fusion for ROS. His thesis is running in parallel to this one, and the solution from his work is therefore not implemented into this project.

% \subsection{Object Detection \& Pick and Place}
% The tag based computer vision system was set up according to section \ref{sec:M:MRC:MachineVision}. The performance of the vision system is verified by testing the combined pick and place performance of the manipulator. Interbotix states a repeatability of 1[mm] for the manipulator\cite{interbotix_vx300}. This tells that large variations in pick accuracy can be attributed to the vision system. It should be mentioned that gripper performance also plays a factor in the pick and place accuracy, as a bad gripper would not be able to achieve consistent results.

% \subsection{Custom ROS2 Packages}
% The custom ROS2 packages has been built and tested around a specific robotic system. It is not likely that it will work for other types of robotic systems out of the box. 


\section{Pick and Place} \label{sec:D:PickAndPlace}
\subsection{Manipulator} \label{sec:D:PAP:Manipulator}
The mounting position of the manipulator, explained in section \ref{sec:M:CompleteHWConfig}, is practical in terms of packaging and keeping the manipulator within the confinements of the UGV while not in use. Nevertheless, the limited reach of the chosen manipulator resulted in a relatively small workspace outside the bounding box of the UGV. For example, the manipulator was unable to reach the ground, meaning that objects either has to be placed on an elevated surface, or the object has to be tall enough to be picked. This is unwanted as it places a higher demand on the accuracy of the autonomous navigation system. Additionally, the limited reach of the manipulator results in less flexibility in terms of object placement at the pick location.

Interbotix reports a payload capacity of $750g$ \cite{interbotix_vx300}, however, with the gripper configuration used during this project, the gripper was barely able to hold the test object, an empty glasses case, with a measured weight of $125g$. The manipulator did not seem to have issues lifting the object, the issue lies in the gripping strength and method of grasping. As described in section \ref{sec:M:A:HuskyPickAndPlace}, the gripper relies on a predefined grasping pose. This is a simple solution, but one that requires precision to be successful. If the grasping pose is too narrow, the gripper servo will overheat, and if the grasping pose is too wide, the gripper will not be able to hold the object. This might not have been such a big issue if the servo motors had brakes, as will be discussed in the next paragraph.

Another consideration regarding the manipulator, is the absence of brakes of any kind in the manipulator's joints. This means that the servo motors constantly have to output a torque to hold a manipulator pose. One side effect of this is unnecessary power drainage, another is the fact that there are no safety mechanisms in place in case of power cut-off or sudden control system failure. In this case, the manipulator could fall uncontrollably and possibly damage itself and/or its environment.

\subsection{Computer Vision Camera}\label{sec:D:PAP:ManipulatorMountedCamera}
For the computer vision camera, it is important that the pose of the camera is accurately defined, and that the physical camera is fixed at the defined pose. The pose of the camera bracket is defined based on CAD measurements and can be assumed to be defined correctly. However, the pose of the camera relative to the bracket might not be accurately defined as this position was not thoroughly verified. As mentioned, the camera bracket can be assumed to be defined correctly, nevertheless, inaccuracies in mounting pose relative to defined pose cannot be ruled out due to possible slack in hole patterns at mounting flange.

\subsection{Scene Geometry Publisher}
The Scene Geometry Publisher described in section \ref{sec:M:PAP:SceneGeometryPublisher}, proved practical in the context of this thesis. It's implementation is specific for this robotic system which means that it is not possible to use this for other robotic manipulators out of the box. However, the ScenePublisher class that the ROS 2 node is built on, is made to be general. Therefore, it is believed that it is possible to adapt this system to other Interbotix manipulators with small changes to the implementation. Additionally, it should be possible to use the class for entirely different robotic manipulators, though that would require a new launch file to be made, as this is heavily inspired by the launch files made by Interbotix.

\subsection{Pick and Place Package}
The Pick and Place Package Described in section \ref{sec:M:A:HuskyPickAndPlace}, is considered to have satisfactory performance. With this package running, the system proved to be robust at responding to the commands given over the custom ROS2 topic \lstinline{/action}. The system is made specifically for the Interbotix VX300 manipulator and it is not believed that it will be easy to adapt this system to other manipulators. 

\subsection{Pick and Place Accuracy}
Pick and place accuracy is not considered during this thesis as the system is not thoroughly enough adjusted. The system is able to detect and pick an object in it's field of view, however, it struggles with large $\theta$(z-axis) rotations when picking. This is most likely be due to inaccuracies when defining the camera placement of the vision system, as Interbotix reports a repeatability of $1[mm]$ for the VX300 robotic arm\cite{interbotix_vx300}. 


\section{Top Level} \label{sec:D:TopLevel}
The "Husky Master" ROS2 node that orchestrates the different behaviours of the system seemed to work as intended. The complete operation where the robot is commanded to move to a pick location, pick an object, move to a place location, and place the object, before navigating back home was performed twice in a row with success on the physical system and twice in a row with success on the gazebo simulation environment of the robotic system. Both these experiments experienced navigational difficulties due to the skidding behaviour of the mobile robot. However, an experiment was also run on a gazebo simulation of the popular differential drive mobile robot TurtleBot3. This system did not experience the same navigational difficulties as the Husky A200 based systems and the result was a much smoother operation. It is important to mention that the TurtleBot might not show as promising results in a physical experiment, though this has not been tested.

The skidding behaviour of the UGV limited the performance of the proposed solution in that it could struggle with some poses. This navigational problem is described in section \ref{sec:D:AN:Navigation}, and this placed a constraint on both on the physical setup, and the simulation of the Husky A200 based system. The constraint meant that the goal poses (pick pose, place pose, home pose) has to be carefully selected so that the robot will not have to reorient itself too much at the goal.

The limited reach of the manipulator, discussed in section \ref{sec:D:PAP:Manipulator}, combined with the rectangular footprint of the UGV could also cause issues. If the robot decides to do sharp turns at pick or place pose, there are chances of collision with objects. This also placed constraints on the warehouse automation system in that it would have to navigate to an intermediate pose, $1[m]$ forward from the pick pose before moving towards the place pose. This was done in order to come clear of the pedestal on which the object to pick was laying.


